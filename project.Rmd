---
title: "Practical Machine Learning Course Project"
author: "Oleksiy Babich"
date: "February 22, 2015"
output: html_document
---

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, the goal is be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data Preprocess

```{r}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
```

An overall random seed was set at 8888 for all code. In order to reproduce the results below, the same seed should be used.

```{r}
set.seed(8888)
```

### Loading data

```{r, cache=TRUE}
training_set <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testing_set <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
```

Some missing values as "#DIV/0!" or "" or "NA" were threated as NA.

### Cleaning data

Remove all missing values columns:
```{r}
training_set <- training_set[, colSums(is.na(training_set)) == 0]
testing_set <- testing_set[, colSums(is.na(testing_set)) == 0]
```

First columns are not needed for our analysis, so we can also remove them getting 53 columns of data instead of initial 160:

```{r}
training_set <-training_set[, -c(1:7)]
testing_set <-testing_set[, -c(1:7)]
```

### Prepare sets for cross-validation

Cross-validation will be performed by subsampling our training data set randomly without replacement into 2 subsamples: `sub_training` (80%) and `sub_testing` (20%).

Models will be fitted on the `sub_training` data, and tested on the `sub_testing` data. Once the most accurate model is choosen, it will be tested on the original Testing data set.

```{r}
sub_samples <- createDataPartition(y=training_set$classe, p=0.8, list=FALSE)
sub_training <- training_set[sub_samples, ] 
sub_testing <- training_set[-sub_samples, ]
```

## Analysis

### First look

The variable “classe” contains 5 levels: A, B, C, D and E.

```{r}
plot(sub_training$classe,  xlab="classe levels", ylab="frequency in sub_training")
```

You can see that me have A as the most frequent `classe`

### Expected out-of-sample error

The expected out-of-sample error will correspond to the quantity: 1-accuracy in the cross-validation data.

Accuracy is the proportion of correct classified observation over the total sample in the `sub_testing` data set. Expected accuracy is the expected accuracy in the out-of-sample data set.

So, the out-of-sample error will correspond to the expected number of missclassified of total observations in the Test data set, which is the quantity, e.g. 1-accuracy found from the cross-validation data set.

Also, I need to mention: `classe` is an unordered factor variable. We may choose error type as 1-accuracy. Large training dataset (19622 obs.) allows to apply cross-validation splitting training set as performed above.

We'll try *decision tree* and *random forest* algorithms which are good in detecting the features that are important for classification.

### First model via Decision Tree

```{r, cache=TRUE}
model_dt <- rpart(classe ~ ., data = sub_training, method = "class")
```

Predicting:
```{r, cache=TRUE}
prediction_dt <- predict(model_dt, sub_testing, type = "class")
```

Plot of the Decision Tree with some extra info under the boxes:
```{r}
rpart.plot(model_dt,
           main = "Classification Tree",
           extra = 102,
           under = TRUE,
           faclen = 0
           )
```

Test results using `sub_testing` data:
```{r}
confusionMatrix(prediction_dt, sub_testing$classe)
```

Please, mention the `Accuracy : 0.7227` and `95% CI : (0.7084, 0.7366)`

### Second model via Random Forest

```{r, cache=TRUE}
model_rf <- randomForest(classe ~. , data = sub_training, method = "class")
```

Predicting:
```{r, cache=TRUE}
prediction_rf <- predict(model_rf, sub_testing, type = "class")
```

Test results using `sub_testing` data:
```{r}
confusionMatrix(prediction_rf, sub_testing$classe)
```
Please, mention the `Accuracy : 0.9972` and `95% CI : (0.995, 0.9986)`

## Conclusion

*Random Forest* is clear winner in this prediction with high Accuracy and 95% Confidence Interval thats why `model_rf` is chosen for submission for `testing_set`.

Epected out-of-sample error (1-accuracy) is estimated at 0.0028, or 0.28%.

## Submission
`testing_set` test data has 20 cases and with so accurate model we expect no errors in prediction.

```{r}
predict_testing <- predict(model_rf, testing_set, type="class")
predict_testing
```

Save results to files for submission:
```{r}
dir.create("pml-testing", showWarnings = FALSE)

for(i in 1:length(predict_testing)) {
  write.table(
    predict_testing[i],
    file = paste0("pml-testing/problem_id_", i ,".txt"),
    quote = FALSE, row.names = FALSE, col.names = FALSE
    )
}
```
